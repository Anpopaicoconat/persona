{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stc/rybin-as/miniconda3/envs/persona/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import transformers\n",
    "\n",
    "from utils import (\n",
    "    PersonaDataset,\n",
    "    GenerativeCollator,\n",
    "    RetrievalCollator,\n",
    "    aggregate_encoder_output,\n",
    "    sim_func,\n",
    ")\n",
    "from models import RetrievalModel, GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proxy\n",
    "os.environ[\"http_proxy\"] = \"http://proxy.ad.speechpro.com:3128\"\n",
    "os.environ[\"https_proxy\"] = \"http://proxy.ad.speechpro.com:3128\"\n",
    "os.environ[\"ftp_proxy\"] = \"http://proxy.ad.speechpro.com:3128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt config\n",
    "parser = argparse.ArgumentParser()\n",
    "gpt_args = parser.parse_args(\"\")\n",
    "with open(\"configs/gpt_config.json\", \"r\") as config:\n",
    "    opt = json.load(config)\n",
    "vars(gpt_args).update(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt tokenizer\n",
    "with open(gpt_args.special_tokens_dict, \"r\") as config:\n",
    "    special_tokens_dict = json.load(config)\n",
    "\n",
    "gpt_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    gpt_args.pretrained_gpt,\n",
    "    truncation_side=gpt_args.truncation_side,\n",
    "    padding_side=gpt_args.padding_side,\n",
    ")\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "gpt_tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 1280)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt\n",
    "gpt = transformers.GPT2LMHeadModel.from_pretrained(gpt_args.pretrained_gpt)\n",
    "gpt.resize_token_embeddings(len(gpt_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "dataset = PersonaDataset(gpt_args.data_path, mod='get_examples_gpt', rnd_context=True)\n",
    "\n",
    "train_size = len(dataset) - len(dataset) // gpt_args.val_split\n",
    "val_size = len(dataset) // gpt_args.val_split\n",
    "vars(gpt_args).update({\"train_size\": train_size, \"val_size\": val_size})\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt callator\n",
    "gpt_callator = GenerativeCollator(\n",
    "    gpt_tokenizer, padding=gpt_args.padding, max_length=gpt_args.max_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=gpt_args.batch_size, shuffle=True, collate_fn=gpt_callator\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=1, shuffle=False, collate_fn=gpt_callator.test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler len\n",
    "scheduler_len = len(train_dataloader) * gpt_args.epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pl trainloop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stc/rybin-as/miniconda3/envs/persona/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:268: UserWarning: Attribute 'GPT' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['GPT'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "# pl model\n",
    "model = GenerativeModel(\n",
    "    gpt,\n",
    "    gpt_tokenizer,\n",
    "    gpt_args.batch_size,\n",
    "    scheduler_len,\n",
    "    gpt_args.num_warmup_steps,\n",
    "    gpt_args.lr,\n",
    "    gpt_args.max_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CometLogger will be initialized in online mode\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/anpopaicoconat/gpt-answer/c36ebc609e3341ccad946566bcb341c9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logger\n",
    "logger = pl.loggers.comet.CometLogger(\n",
    "    api_key=gpt_args.api_key,\n",
    "    save_dir=gpt_args.save_dir,\n",
    "    project_name=gpt_args.project_name,\n",
    "    experiment_name=gpt_args.experiment_name,\n",
    ")\n",
    "logger.log_hyperparams(gpt_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint callback\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "     monitor='val_loss',\n",
    "     dirpath=gpt_args.save_dir,\n",
    "     filename='gpt-{epoch:02d}-{val_loss:.2f}',\n",
    "     save_top_k=1,\n",
    "     mode='min',\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=gpt_args.epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    gradient_clip_val=gpt_args.gradient_clip_val,\n",
    "    logger=logger,\n",
    "    num_sanity_val_steps=1,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('persona')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12e871139975605d27e2df52837a3758456bf52e5574476cc04e0ddd66d30be3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
