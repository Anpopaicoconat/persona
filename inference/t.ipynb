{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stc/rybin-as/miniconda3/envs/persona/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import transformers\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils import (\n",
    "    GenerativeCollator,\n",
    "    RetrievalCollator,\n",
    ")\n",
    "from models import BERT_RetrievalModel, GPT_GenerativeModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiEncoder_GPT:\n",
    "    def __init__(\n",
    "        self,\n",
    "        retrieval_model,\n",
    "        generative_model,\n",
    "    ):\n",
    "        self.retrieval_model = retrieval_model\n",
    "        self.generative_model = generative_model\n",
    "\n",
    "    def calculate_candidats(self, candidats_texts: list[str]) -> torch.Tensor:\n",
    "        \"\"\"расчитывает векторную базу кандидатов\n",
    "\n",
    "        Args:\n",
    "            candidats_texts (list[str]): список фактов о персоне\n",
    "        Returns:\n",
    "            torch.Tensor: вектора фактов о персоне\n",
    "        \"\"\"\n",
    "        candidats_tokens = self.retrieval_model.collator.CandidateCollator(\n",
    "            candidats_texts\n",
    "        )\n",
    "        candidats_vec = self.retrieval_model.encode_candidats(candidats_tokens)\n",
    "        return candidats_vec\n",
    "\n",
    "    def retrieve_gk(\n",
    "        self,\n",
    "        context_texts: list[str],\n",
    "        candidats_texts,\n",
    "        candidats_vecs: list[torch.Tensor],\n",
    "    ) -> list[str]:\n",
    "        \"\"\"находит релевантные контексту кандидатов\n",
    "\n",
    "        Args:\n",
    "            context_texts (str): _description_\n",
    "            candidats_vec (torch.Tensor): _description_\n",
    "\n",
    "        Returns:\n",
    "            list[str]: список кандидатов\n",
    "        \"\"\"\n",
    "        context_texts = context_texts[-1][1]\n",
    "        context_tokens = self.retrieval_model.collator.ContextCollator(\n",
    "            [[context_texts]]\n",
    "        )\n",
    "        context_vec = self.retrieval_model.encode_context(context_tokens)\n",
    "        candidats_vecs = torch.tensor(candidats_vecs)\n",
    "        context_vec = context_vec.repeat(candidats_vecs.size()[0], 1)\n",
    "        distances = self.retrieval_model.compute_sim(context_vec, candidats_vecs)[\n",
    "            0\n",
    "        ].tolist()\n",
    "        all_candidats = sorted(\n",
    "            list(zip(distances, candidats_texts)), key=lambda x: x[0], reverse=True\n",
    "        )\n",
    "        candidats = [(d, c) for d, c in all_candidats[:3] if d > 1]\n",
    "        return candidats, all_candidats\n",
    "\n",
    "    def generate_reply(self, context_texts, gks):\n",
    "        # TODO: расширить регулярки\n",
    "        context_texts = [i[1] for i in context_texts]\n",
    "        gks = [i[1] for i in gks]\n",
    "        dict_inp = [{\"context\": context_texts, \"gk\": gks, \"candidate\": \"\"}]\n",
    "        gpt_inp = self.generative_model.collator.test(dict_inp)[0][\"input_ids\"][:, :-2]\n",
    "        gpt_out = self.generative_model.GPT.generate(\n",
    "            gpt_inp,\n",
    "            max_new_tokens=32,\n",
    "        )\n",
    "        gpt_out = self.generative_model.tokenizer.decode(\n",
    "            gpt_out[0][-32:], skip_special_tokens=False\n",
    "        )\n",
    "        gpt_out = gpt_out.split(\"[Gk]\")\n",
    "        msg = (\n",
    "            gpt_out[-1].split(\"[P2u]\")[1].split(\"[P1u]\")[0].replace(\"<|endoftext|>\", \"\")\n",
    "        )\n",
    "        new_gks = gpt_out[:-1]\n",
    "        return msg, new_gks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_encoder = BERT_RetrievalModel.load_from_checkpoint('/home/stc/persona/logs/bi_encoder/36037371cee4404b80aa618268a2e24c/checkpoints/epoch=29-step=22080.ckpt')\n",
    "bi_encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative = GPT_GenerativeModel.load_from_checkpoint('/home/stc/persona/logs/gpt-epoch=00-val_loss=3.62.ckpt')\n",
    "generative.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = BiEncoder_GPT(\n",
    "    retrieval_model=bi_encoder,\n",
    "    generative_model=generative,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3298/4141479128.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  candidats_vecs = torch.tensor(candidats_vecs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidats_texts=['я маша', 'я работаю садовником', 'я люблю кофе', 'я люблю собак', 'я люблю кошек', 'я бегаю по утрам']\n",
    "context_texts = [('user', 'привет'), ('model', 'привет'), ('user', 'ты занимаешься спортом?')]\n",
    "candidats_vecs = full_model.calculate_candidats(candidats_texts)\n",
    "gks = full_model.retrieve_gk(context_texts, candidats_texts, candidats_vecs)[0]\n",
    "gks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P2u]не[P1u]я хожу в фитнес-клуб[P2u]это хорошо[P1u]а ты?[P2u]да, люблю лыжи[P1u]круто[P2u]ты любишь готовить?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('не', [])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model.generate_reply(context_texts, gks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('persona')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12e871139975605d27e2df52837a3758456bf52e5574476cc04e0ddd66d30be3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
