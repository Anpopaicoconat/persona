{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stc/rybin-as/miniconda3/envs/persona/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import transformers\n",
    "\n",
    "from utils import (\n",
    "    PersonaDataset,\n",
    "    GenerativeCollator,\n",
    "    RetrievalCollator,\n",
    "    aggregate_encoder_output,\n",
    "    sim_func,\n",
    ")\n",
    "from models import RetrievalModel, GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proxy\n",
    "os.environ[\"http_proxy\"] = \"http://proxy.ad.speechpro.com:3128\"\n",
    "os.environ[\"https_proxy\"] = \"http://proxy.ad.speechpro.com:3128\"\n",
    "os.environ[\"ftp_proxy\"] = \"http://proxy.ad.speechpro.com:3128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config bert\n",
    "parser = argparse.ArgumentParser()\n",
    "bert_args = parser.parse_args(\"\")\n",
    "with open(\"configs/bert_config.json\", \"r\") as config:\n",
    "    opt = json.load(config)\n",
    "vars(bert_args).update(opt)\n",
    "\n",
    "# config gpt\n",
    "parser = argparse.ArgumentParser()\n",
    "gpt_args = parser.parse_args(\"\")\n",
    "with open(\"configs/gpt_config.json\", \"r\") as config:\n",
    "    opt = json.load(config)\n",
    "vars(gpt_args).update(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(bert_args.special_tokens_dict, \"r\") as config:\n",
    "    special_tokens_dict = json.load(config)\n",
    "\n",
    "# bert tokenizer\n",
    "bert_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    bert_args.pretrained_bert,\n",
    "    truncation_side=bert_args.truncation_side,\n",
    "    padding_side=bert_args.padding_side,\n",
    ")\n",
    "bert_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# gpt tokenizer\n",
    "gpt_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    gpt_args.pretrained_gpt,\n",
    "    truncation_side=gpt_args.truncation_side,\n",
    "    padding_side=gpt_args.padding_side,\n",
    ")\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "gpt_tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieva model\n",
    "bert_ckpt = \"/home/stc/persona/logs/bi_encoder/ba9a2503126b46e2b2ec8049c669b0f1/checkpoints/epoch=29-step=22770.ckpt\"\n",
    "retrieval_model = RetrievalModel.load_from_checkpoint(bert_ckpt)\n",
    "retrieval_model.eval()\n",
    "\n",
    "# generative model\n",
    "gpt_ckpt = \"logs/gpt_answer/gpt-epoch=02-val_loss=0.61.ckpt\"\n",
    "generative_model = GenerativeModel.load_from_checkpoint(gpt_ckpt)\n",
    "generative_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "# bert callator\n",
    "bert_callator = RetrievalCollator(\n",
    "    bert_tokenizer, padding=bert_args.padding, max_length=bert_args.context_len\n",
    ")\n",
    "\n",
    "# gpt callator\n",
    "gpt_callator = GenerativeCollator(\n",
    "    gpt_tokenizer, padding=gpt_args.padding, max_length=gpt_args.max_len\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode functions\n",
    "def encode_persona(text_batch, encoder):\n",
    "    inp_persona_tokens = bert_callator.CandidateCollator(text_batch)\n",
    "    vec_batch = aggregate_encoder_output(\n",
    "        encoder.candidat_BERT(**inp_persona_tokens), mod=bert_args.aggregation_mod\n",
    "    )\n",
    "    return vec_batch\n",
    "\n",
    "\n",
    "def encode_context(text_batch, encoder):\n",
    "    inp_context_tokens = bert_callator.ContextCollator([text_batch])\n",
    "    print(bert_tokenizer.batch_decode(inp_context_tokens['input_ids']))\n",
    "    vec_batch = aggregate_encoder_output(\n",
    "        encoder.context_BERT(**inp_context_tokens), mod=bert_args.aggregation_mod\n",
    "    )\n",
    "    return vec_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona = [\n",
    "    \"У меня любимая работа\",\n",
    "    \"Я уважаю людей\",\n",
    "    \"У меня есть попугай\",\n",
    "    \"Я был в Париже\",\n",
    "    \"Я люблю кофе\",\n",
    "    \"У меня есть собака\",\n",
    "    \"У меня есть кошка\",\n",
    "    \"Я играю на гитаре\",\n",
    "    \"Я кассир\",\n",
    "    \"Я работаю в магазине\",\n",
    "]\n",
    "context = []\n",
    "\n",
    "vec_persona = encode_persona(persona, retrieval_model)\n",
    "# vec_context = encode_context(context, retrieval_model)\n",
    "# ranks = sim_func(vec_context, vec_persona, mod=bert_args.sim_mod)[0].tolist()\n",
    "# gks = sorted(list(zip(ranks, persona)), key=lambda x: x[0], reverse=True)\n",
    "# print(gks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- а как ты относишься к собакам собакам собакам собакам собакам?\n",
      "['[CLS] [P1u] а как ты относишься к собакам собакам собакам собакам собакам? [P2u] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n",
      "знания о персоне: [(0.6285446882247925, 'У меня есть собака.'), (-2.215956926345825, 'У меня есть кошка.'), (-5.754154682159424, 'У меня любимая работа.'), (-6.45443058013916, 'У меня есть попугай.'), (-9.956965446472168, 'Я играю на гитаре.'), (-10.478581428527832, 'Я работаю в магазине.'), (-10.619494438171387, 'Я кассир.'), (-12.453767776489258, 'Я люблю кофе.'), (-13.461600303649902, 'Я был в Париже.'), (-16.636831283569336, 'Я уважаю людей.')]\n",
      "\n",
      "         ([P1u]а как ты относишься к собакам собакам собакам собакам собакам?[Gk]У меня есть собака.)\n",
      "\n",
      "\n",
      "         ([P1u]а как ты относишься к собакам собакам собакам собакам собакам?[Gk]У меня есть собака.[P2u]У меня есть собака, зовут малыш, правда весит он как слон кг 70, я его на улице нашла Очень жаль, я бы не смогла с ним)\n",
      "\n",
      "model: У меня есть собака, зовут малыш, правда весит он как слон кг 70, я его на улице нашла Очень жаль, я бы не смогла с ним\n"
     ]
    }
   ],
   "source": [
    "user_msg = '?'\n",
    "context.append(user_msg)\n",
    "for c in context:\n",
    "    print('-',c)\n",
    "vec_context = encode_context(context[-2:], retrieval_model)\n",
    "ranks = sim_func(vec_context, vec_persona, mod=bert_args.sim_mod)[0].tolist()\n",
    "gks = sorted(list(zip(ranks, persona)), key=lambda x: x[0], reverse=True)\n",
    "print(\"знания о персоне:\", gks)\n",
    "gks = [gk[1] for gk in gks[:1]]\n",
    "\n",
    "# generate\n",
    "dict_inp = [{\"context\": context, \"gk\": gks, \"candidate\": \"\"}]\n",
    "gpt_inp = gpt_callator.test(dict_inp)[0][\"input_ids\"][:, :-2]\n",
    "len_gpt_inp = gpt_inp.size()[-1]\n",
    "print()\n",
    "print((\"         (\" + gpt_tokenizer.batch_decode(gpt_inp)[0] + \")\"))\n",
    "print()\n",
    "gpt_out = generative_model.GPT.generate(\n",
    "    gpt_inp,\n",
    "    max_new_tokens=32,\n",
    ")\n",
    "gpt_answer = gpt_out\n",
    "answer_raw = gpt_tokenizer.decode(gpt_answer[0], skip_special_tokens=False)\n",
    "print()\n",
    "print((\"         (\" + answer_raw + \")\"))\n",
    "print()\n",
    "\n",
    "# proc answer\n",
    "answer = gpt_tokenizer.decode(\n",
    "    gpt_answer[0, len_gpt_inp + 1 :], skip_special_tokens=True\n",
    ")\n",
    "context.append(answer)\n",
    "print(\"model:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('persona')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12e871139975605d27e2df52837a3758456bf52e5574476cc04e0ddd66d30be3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
