{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import transformers\n",
    "\n",
    "from utils import (\n",
    "    PersonaDataset,\n",
    "    GenerativeCollator,\n",
    "    RetrievalCollator,\n",
    "    aggregate_encoder_output,\n",
    "    sim_func,\n",
    ")\n",
    "from models import GPT_GenerativeModel\n",
    "\n",
    "pl.utilities.seed.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proxy\n",
    "os.environ[\"http_proxy\"] = \"http://proxy.ad.speechpro.com:3128\"\n",
    "os.environ[\"https_proxy\"] = \"http://proxy.ad.speechpro.com:3128\"\n",
    "os.environ[\"ftp_proxy\"] = \"http://proxy.ad.speechpro.com:3128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt config\n",
    "parser = argparse.ArgumentParser()\n",
    "gpt_args = parser.parse_args(\"\")\n",
    "with open(\"configs/gpt_config.json\", \"r\") as config:\n",
    "    opt = json.load(config)\n",
    "vars(gpt_args).update(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "parser = argparse.ArgumentParser()\n",
    "gpt_args = parser.parse_args(\"\")\n",
    "with open(\"../config.json\", \"r\") as config:\n",
    "    opt = json.load(config)\n",
    "vars(gpt_args).update(opt)\n",
    "\n",
    "opt = {\n",
    "    \"epochs\": 3,\n",
    "    \"lr\": 5e-05,\n",
    "    \"gradient_clip_val\": 1,\n",
    "    \"batch_size\": 8,\n",
    "    \"val_split\": -1,\n",
    "    \"max_len\": 128,\n",
    "    \"num_warmup_steps\": 1500,\n",
    "    \"project_name\": \"gpt_answer\",\n",
    "    \"experiment_name\": \"context+gks>answer(5e-05)\",\n",
    "    \"dataset_mod\": \"get_examples_gpt\",\n",
    "    \"rnd_context\": 0,\n",
    "}\n",
    "vars(gpt_args).update(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt tokenizer\n",
    "gpt_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    gpt_args.gpt,\n",
    "    truncation_side=gpt_args.truncation_side,\n",
    "    padding_side=gpt_args.padding_side,\n",
    ")\n",
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
    "gpt_tokenizer.add_special_tokens(gpt_args.special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 1280)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt\n",
    "gpt = transformers.GPT2LMHeadModel.from_pretrained(gpt_args.gpt)\n",
    "gpt.resize_token_embeddings(len(gpt_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143156 1000\n"
     ]
    }
   ],
   "source": [
    "# dataset\n",
    "train_dataset = PersonaDataset(gpt_args.train_data_path, mod=gpt_args.dataset_mod, rnd_context=gpt_args.rnd_context)\n",
    "val_dataset = PersonaDataset(gpt_args.test_data_path, mod=gpt_args.dataset_mod, rnd_context=gpt_args.rnd_context)[:1000]\n",
    "train_size = len(train_dataset)\n",
    "val_size = len(val_dataset)\n",
    "vars(gpt_args).update({\"train_size\": train_size, \"val_size\": val_size})\n",
    "print(train_size, val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt callator\n",
    "gpt_callator = GenerativeCollator(\n",
    "    gpt_tokenizer, padding=gpt_args.padding, max_length=gpt_args.max_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=gpt_args.batch_size, shuffle=True, collate_fn=gpt_callator\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=1, shuffle=False, collate_fn=gpt_callator.test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler len\n",
    "scheduler_len = len(train_dataloader) * gpt_args.epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pl trainloop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stc/rybin-as/miniconda3/envs/persona/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:268: UserWarning: Attribute 'GPT' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['GPT'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "# pl model\n",
    "model = GPT_GenerativeModel(\n",
    "    GPT=gpt,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    batch_size=gpt_args.batch_size,\n",
    "    scheduler_len=scheduler_len,\n",
    "    num_warmup_steps=gpt_args.num_warmup_steps,\n",
    "    lr=gpt_args.lr,\n",
    "    max_len=gpt_args.max_len,\n",
    "    collator=gpt_callator,\n",
    "    base_config=gpt_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CometLogger will be initialized in online mode\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/anpopaicoconat/gpt-answer/2ba72f4292404b8abf505cea8cd77991\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# logger\n",
    "logger = pl.loggers.comet.CometLogger(\n",
    "    api_key=gpt_args.api_key,\n",
    "    save_dir=gpt_args.save_dir,\n",
    "    project_name=gpt_args.project_name,\n",
    "    experiment_name=gpt_args.experiment_name,\n",
    ")\n",
    "logger.log_hyperparams(gpt_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint callback\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "     monitor='val_loss',\n",
    "     dirpath=gpt_args.save_dir,\n",
    "     filename='gpt-{epoch:02d}-{val_loss:.2f}',\n",
    "     save_top_k=1,\n",
    "     mode='min',\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=gpt_args.epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    gradient_clip_val=gpt_args.gradient_clip_val,\n",
    "    logger=logger,\n",
    "    num_sanity_val_steps=1,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('persona')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12e871139975605d27e2df52837a3758456bf52e5574476cc04e0ddd66d30be3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
